{"cells": [{"metadata": {"id": "3a84dd0791f54f2eba25d609d4f4f322"}, "cell_type": "markdown", "source": "# Install OpenVINO in IBM Cloud Pak for Data - Watson Studio Jupyter Env"}, {"metadata": {"id": "7450cea6960b437f8309beea421ddf4b"}, "cell_type": "markdown", "source": "### Use Python 3.6 in IBM CP4D Watson Studio Jupyter Env\n### OpenVINO currently supports Python 3.6, 3.8 on Red Hat* Enterprise Linux* 8, 64-bit\t\n"}, {"metadata": {"id": "f654368fde164285a10ef2c79811d8cb"}, "cell_type": "markdown", "source": "### Resources:\n1. OpenVINO PyPi: https://pypi.org/project/openvino-dev/\n2. IBM CP4D: [Customizing environment definitions (Watson Studio)](https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/wsj/analyze-data/cust-env-parent.html)"}, {"metadata": {"id": "7aff8282-127b-43aa-8738-c0096a11e805"}, "cell_type": "code", "source": "# Install this specific version of OpenCV to prevent libGl errors\n!pip uninstall -y opencv-python\n!pip install -U opencv-python-headless==4.2.0.32 --user", "execution_count": null, "outputs": []}, {"metadata": {"id": "ef47c5b9189a4e9a9207d4d88861ce89"}, "cell_type": "code", "source": "# Install OpenVINO\n!pip install --ignore-installed PyYAML openvino-dev", "execution_count": null, "outputs": []}, {"metadata": {"id": "8f19bcb3f1954e1c80746c937f1329fc", "scrolled": true}, "cell_type": "code", "source": "!pip show openvino", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Name: openvino\r\nVersion: 2021.4.0\r\nSummary: Inference Engine Python* API\r\nHome-page: https://docs.openvinotoolkit.org/latest/index.html\r\nAuthor: Intel Corporation\r\nAuthor-email: openvino_pushbot@intel.com\r\nLicense: Proprietary - Intel\r\nLocation: /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages\r\nRequires: numpy\r\nRequired-by: openvino-dev\r\n", "name": "stdout"}]}, {"metadata": {"id": "05d697550317482189bbcdd29477ecb2"}, "cell_type": "markdown", "source": "# After installing, Restart Kernel just to be sure..."}, {"metadata": {"id": "0bdb111a86d24c9885985ba7d91268d6"}, "cell_type": "markdown", "source": "### Test OpenVINO imports"}, {"metadata": {"id": "f5ebc3e9eadc40ca8716d1dacc6489c3"}, "cell_type": "code", "source": "from openvino.inference_engine import IENetwork, IECore\nfrom openvino.tools.benchmark.main import main", "execution_count": 4, "outputs": []}, {"metadata": {"id": "cf04e2e81b1049e383b0bb62db50fb79"}, "cell_type": "markdown", "source": "### Test Model Optimizer"}, {"metadata": {"id": "615ca08063454ee59d5d3a0afacf6ac7"}, "cell_type": "code", "source": "!mo -h", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "usage: main.py [options]\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --framework {tf,caffe,mxnet,kaldi,onnx}\r\n                        Name of the framework used to train the input model.\r\n\r\nFramework-agnostic parameters:\r\n  --input_model INPUT_MODEL, -w INPUT_MODEL, -m INPUT_MODEL\r\n                        Tensorflow*: a file with a pre-trained model (binary\r\n                        or text .pb file after freezing). Caffe*: a model\r\n                        proto file with model weights\r\n  --model_name MODEL_NAME, -n MODEL_NAME\r\n                        Model_name parameter passed to the final create_ir\r\n                        transform. This parameter is used to name a network in\r\n                        a generated IR and output .xml/.bin files.\r\n  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\r\n                        Directory that stores the generated IR. By default, it\r\n                        is the directory from where the Model Optimizer is\r\n                        launched.\r\n  --input_shape INPUT_SHAPE\r\n                        Input shape(s) that should be fed to an input node(s)\r\n                        of the model. Shape is defined as a comma-separated\r\n                        list of integer numbers enclosed in parentheses or\r\n                        square brackets, for example [1,3,227,227] or\r\n                        (1,227,227,3), where the order of dimensions depends\r\n                        on the framework input layout of the model. For\r\n                        example, [N,C,H,W] is used for Caffe* models and\r\n                        [N,H,W,C] for TensorFlow* models. Model Optimizer\r\n                        performs necessary transformations to convert the\r\n                        shape to the layout required by Inference Engine\r\n                        (N,C,H,W). The shape should not contain undefined\r\n                        dimensions (? or -1) and should fit the dimensions\r\n                        defined in the input operation of the graph. If there\r\n                        are multiple inputs in the model, --input_shape should\r\n                        contain definition of shape for each input separated\r\n                        by a comma, for example: [1,3,227,227],[2,4] for a\r\n                        model with two inputs with 4D and 2D shapes.\r\n                        Alternatively, specify shapes with the --input option.\r\n  --scale SCALE, -s SCALE\r\n                        All input values coming from original network inputs\r\n                        will be divided by this value. When a list of inputs\r\n                        is overridden by the --input parameter, this scale is\r\n                        not applied for any input that does not match with the\r\n                        original input of the model.\r\n  --reverse_input_channels\r\n                        Switch the input channels order from RGB to BGR (or\r\n                        vice versa). Applied to original inputs of the model\r\n                        if and only if a number of channels equals 3. Applied\r\n                        after application of --mean_values and --scale_values\r\n                        options, so numbers in --mean_values and\r\n                        --scale_values go in the order of channels used in the\r\n                        original model.\r\n  --log_level {CRITICAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}\r\n                        Logger level\r\n  --input INPUT         Quoted list of comma-separated input nodes names with\r\n                        shapes, data types, and values for freezing. The shape\r\n                        and value are specified as space-separated lists. The\r\n                        data type of input node is specified in braces and can\r\n                        have one of the values: f64 (float64), f32 (float32),\r\n                        f16 (float16), i64 (int64), i32 (int32), u8 (uint8),\r\n                        boolean. For example, use the following format to set\r\n                        input port 0 of the node `node_name1` with the shape\r\n                        [3 4] as an input node and freeze output port 1 of the\r\n                        node `node_name2` with the value [20 15] of the int32\r\n                        type and shape [2]: \"0:node_name1[3\r\n                        4],node_name2:1[2]{i32}->[20 15]\".\r\n  --output OUTPUT       The name of the output operation of the model. For\r\n                        TensorFlow*, do not add :0 to this name.\r\n  --mean_values MEAN_VALUES, -ms MEAN_VALUES\r\n                        Mean values to be used for the input image per\r\n                        channel. Values to be provided in the (R,G,B) or\r\n                        [R,G,B] format. Can be defined for desired input of\r\n                        the model, for example: \"--mean_values\r\n                        data[255,255,255],info[255,255,255]\". The exact\r\n                        meaning and order of channels depend on how the\r\n                        original model was trained.\r\n  --scale_values SCALE_VALUES\r\n                        Scale values to be used for the input image per\r\n                        channel. Values are provided in the (R,G,B) or [R,G,B]\r\n                        format. Can be defined for desired input of the model,\r\n                        for example: \"--scale_values\r\n                        data[255,255,255],info[255,255,255]\". The exact\r\n                        meaning and order of channels depend on how the\r\n                        original model was trained.\r\n  --data_type {FP16,FP32,half,float}\r\n                        Data type for all intermediate tensors and weights. If\r\n                        original model is in FP32 and --data_type=FP16 is\r\n                        specified, all model weights and biases are quantized\r\n                        to FP16.\r\n  --transform TRANSFORM\r\n                        Apply additional transformations. Usage: \"--transform\r\n                        transformation_name1[args],transformation_name2...\"\r\n                        where [args] is key=value pairs separated by\r\n                        semicolon. Examples: \"--transform LowLatency2\" or \"--\r\n                        transform LowLatency2[use_const_initializer=False]\"\r\n                        Available transformations: \"LowLatency2\"\r\n  --disable_fusing      Turn off fusing of linear operations to Convolution\r\n  --disable_resnet_optimization\r\n                        Turn off resnet optimization\r\n  --finegrain_fusing FINEGRAIN_FUSING\r\n                        Regex for layers/operations that won't be fused.\r\n                        Example: --finegrain_fusing Convolution1,.*Scale.*\r\n  --disable_gfusing     Turn off fusing of grouped convolutions\r\n  --enable_concat_optimization\r\n                        Turn on Concat optimization.\r\n  --move_to_preprocess  Move mean values to IR preprocess section\r\n  --extensions EXTENSIONS\r\n                        Directory or a comma separated list of directories\r\n                        with extensions. To disable all extensions including\r\n                        those that are placed at the default location, pass an\r\n                        empty string.\r\n  --batch BATCH, -b BATCH\r\n                        Input batch size\r\n  --version             Version of Model Optimizer\r\n  --silent              Prevent any output messages except those that\r\n                        correspond to log level equals ERROR, that can be set\r\n                        with the following option: --log_level. By default,\r\n                        log level is already ERROR.\r\n  --freeze_placeholder_with_value FREEZE_PLACEHOLDER_WITH_VALUE\r\n                        Replaces input layer with constant node with provided\r\n                        value, for example: \"node_name->True\". It will be\r\n                        DEPRECATED in future releases. Use --input option to\r\n                        specify a value for freezing.\r\n  --generate_deprecated_IR_V7\r\n                        Force to generate deprecated IR V7 with layers from\r\n                        old IR specification.\r\n  --static_shape        Enables IR generation for fixed input shape (folding\r\n                        `ShapeOf` operations and shape-calculating sub-graphs\r\n                        to `Constant`). Changing model input shape using the\r\n                        Inference Engine API in runtime may fail for such an\r\n                        IR.\r\n  --keep_shape_ops      The option is ignored. Expected behavior is enabled by\r\n                        default.\r\n  --disable_weights_compression\r\n                        Disable compression and store weights with original\r\n                        precision.\r\n  --progress            Enable model conversion progress display.\r\n  --stream_output       Switch model conversion progress display to a\r\n                        multiline mode.\r\n  --transformations_config TRANSFORMATIONS_CONFIG\r\n                        Use the configuration file with transformations\r\n                        description.\r\n  --legacy_ir_generation\r\n                        Use legacy IR serialization engine\r\n\r\nTensorFlow*-specific parameters:\r\n  --input_model_is_text\r\n                        TensorFlow*: treat the input model file as a text\r\n                        protobuf format. If not specified, the Model Optimizer\r\n                        treats it as a binary file by default.\r\n  --input_checkpoint INPUT_CHECKPOINT\r\n                        TensorFlow*: variables file to load.\r\n  --input_meta_graph INPUT_META_GRAPH\r\n                        Tensorflow*: a file with a meta-graph of the model\r\n                        before freezing\r\n  --saved_model_dir SAVED_MODEL_DIR\r\n                        TensorFlow*: directory with a model in SavedModel\r\n                        formatof TensorFlow 1.x or 2.x version.\r\n  --saved_model_tags SAVED_MODEL_TAGS\r\n                        Group of tag(s) of the MetaGraphDef to load, in string\r\n                        format, separated by ','. For tag-set contains\r\n                        multiple tags, all tags must be passed in.\r\n  --tensorflow_custom_operations_config_update TENSORFLOW_CUSTOM_OPERATIONS_CONFIG_UPDATE\r\n                        TensorFlow*: update the configuration file with node\r\n                        name patterns with input/output nodes information.\r\n  --tensorflow_use_custom_operations_config TENSORFLOW_USE_CUSTOM_OPERATIONS_CONFIG\r\n                        Use the configuration file with custom operation\r\n                        description.\r\n  --tensorflow_object_detection_api_pipeline_config TENSORFLOW_OBJECT_DETECTION_API_PIPELINE_CONFIG\r\n                        TensorFlow*: path to the pipeline configuration file\r\n                        used to generate model created with help of Object\r\n                        Detection API.\r\n  --tensorboard_logdir TENSORBOARD_LOGDIR\r\n                        TensorFlow*: dump the input graph to a given directory\r\n                        that should be used with TensorBoard.\r\n  --tensorflow_custom_layer_libraries TENSORFLOW_CUSTOM_LAYER_LIBRARIES\r\n                        TensorFlow*: comma separated list of shared libraries\r\n                        with TensorFlow* custom operations implementation.\r\n  --disable_nhwc_to_nchw\r\n                        Disables default translation from NHWC to NCHW\r\n\r\nCaffe*-specific parameters:\r\n  --input_proto INPUT_PROTO, -d INPUT_PROTO\r\n                        Deploy-ready prototxt file that contains a topology\r\n                        structure and layer attributes\r\n  --caffe_parser_path CAFFE_PARSER_PATH\r\n                        Path to Python Caffe* parser generated from\r\n                        caffe.proto\r\n  -k K                  Path to CustomLayersMapping.xml to register custom\r\n                        layers\r\n  --mean_file MEAN_FILE, -mf MEAN_FILE\r\n                        Mean image to be used for the input. Should be a\r\n                        binaryproto file\r\n  --mean_file_offsets MEAN_FILE_OFFSETS, -mo MEAN_FILE_OFFSETS\r\n                        Mean image offsets to be used for the input\r\n                        binaryproto file. When the mean image is bigger than\r\n                        the expected input, it is cropped. By default, centers\r\n                        of the input image and the mean image are the same and\r\n                        the mean image is cropped by dimensions of the input\r\n                        image. The format to pass this option is the\r\n                        following: \"-mo (x,y)\". In this case, the mean file is\r\n                        cropped by dimensions of the input image with offset\r\n                        (x,y) from the upper left corner of the mean image\r\n  --disable_omitting_optional\r\n                        Disable omitting optional attributes to be used for\r\n                        custom layers. Use this option if you want to transfer\r\n                        all attributes of a custom layer to IR. Default\r\n                        behavior is to transfer the attributes with default\r\n                        values and the attributes defined by the user to IR.\r\n  --enable_flattening_nested_params\r\n                        Enable flattening optional params to be used for\r\n                        custom layers. Use this option if you want to transfer\r\n                        attributes of a custom layer to IR with flattened\r\n                        nested parameters. Default behavior is to transfer the\r\n                        attributes without flattening nested parameters.\r\n\r\nMxnet-specific parameters:\r\n  --input_symbol INPUT_SYMBOL\r\n                        Symbol file (for example, model-symbol.json) that\r\n                        contains a topology structure and layer attributes\r\n  --nd_prefix_name ND_PREFIX_NAME\r\n                        Prefix name for args.nd and argx.nd files.\r\n  --pretrained_model_name PRETRAINED_MODEL_NAME\r\n                        Name of a pretrained MXNet model without extension and\r\n                        epoch number. This model will be merged with args.nd\r\n                        and argx.nd files\r\n  --save_params_from_nd\r\n                        Enable saving built parameters file from .nd files\r\n  --legacy_mxnet_model  Enable MXNet loader to make a model compatible with\r\n                        the latest MXNet version. Use only if your model was\r\n                        trained with MXNet version lower than 1.0.0\r\n  --enable_ssd_gluoncv  Enable pattern matchers replacers for converting\r\n                        gluoncv ssd topologies.\r\n\r\nKaldi-specific parameters:\r\n  --counts COUNTS       Path to the counts file\r\n  --remove_output_softmax\r\n                        Removes the SoftMax layer that is the output layer\r\n  --remove_memory       Removes the Memory layer and use additional inputs\r\n                        outputs instead\r\n", "name": "stdout"}]}, {"metadata": {"id": "71fd9d4272f840d3b41c5c90d20ad4b1"}, "cell_type": "markdown", "source": "### Test Benchmark App"}, {"metadata": {"id": "681bb6c8b36e40b68b05311697cb3d1f"}, "cell_type": "code", "source": "!benchmark_app -h", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "[Step 1/11] Parsing and validating input arguments\r\nusage: benchmark_app [-h [HELP]] [-i PATHS_TO_INPUT [PATHS_TO_INPUT ...]] -m\r\n                     PATH_TO_MODEL [-d TARGET_DEVICE] [-l PATH_TO_EXTENSION]\r\n                     [-c PATH_TO_CLDNN_CONFIG] [-api {sync,async}]\r\n                     [-niter NUMBER_ITERATIONS] [-nireq NUMBER_INFER_REQUESTS]\r\n                     [-b BATCH_SIZE] [-stream_output [STREAM_OUTPUT]]\r\n                     [-t TIME] [-progress [PROGRESS]] [-shape SHAPE]\r\n                     [-layout LAYOUT] [-nstreams NUMBER_STREAMS]\r\n                     [-enforcebf16 [{True,False}]] [-nthreads NUMBER_THREADS]\r\n                     [-pin {YES,NO,NUMA,HYBRID_AWARE}]\r\n                     [-exec_graph_path EXEC_GRAPH_PATH] [-pc [PERF_COUNTS]]\r\n                     [-report_type {no_counters,average_counters,detailed_counters}]\r\n                     [-report_folder REPORT_FOLDER] [-dump_config DUMP_CONFIG]\r\n                     [-load_config LOAD_CONFIG] [-qb {8,16}]\r\n                     [-ip {U8,FP16,FP32}] [-op {U8,FP16,FP32}]\r\n                     [-iop INPUT_OUTPUT_PRECISION] [-cdir CACHE_DIR]\r\n                     [-lfile [LOAD_FROM_FILE]]\r\n\r\nOptions:\r\n  -h [HELP], --help [HELP]\r\n                        Show this help message and exit.\r\n  -i PATHS_TO_INPUT [PATHS_TO_INPUT ...], --paths_to_input PATHS_TO_INPUT [PATHS_TO_INPUT ...]\r\n                        Optional. Path to a folder with images and/or binaries\r\n                        or to specific image or binary file.\r\n  -m PATH_TO_MODEL, --path_to_model PATH_TO_MODEL\r\n                        Required. Path to an .xml/.onnx/.prototxt file with a\r\n                        trained model or to a .blob file with a trained\r\n                        compiled model.\r\n  -d TARGET_DEVICE, --target_device TARGET_DEVICE\r\n                        Optional. Specify a target device to infer on (the\r\n                        list of available devices is shown below). Default\r\n                        value is CPU. Use '-d HETERO:<comma separated devices\r\n                        list>' format to specify HETERO plugin. Use '-d\r\n                        MULTI:<comma separated devices list>' format to\r\n                        specify MULTI plugin. The application looks for a\r\n                        suitable plugin for the specified device.\r\n  -l PATH_TO_EXTENSION, --path_to_extension PATH_TO_EXTENSION\r\n                        Optional. Required for CPU custom layers. Absolute\r\n                        path to a shared library with the kernels\r\n                        implementations.\r\n  -c PATH_TO_CLDNN_CONFIG, --path_to_cldnn_config PATH_TO_CLDNN_CONFIG\r\n                        Optional. Required for GPU custom kernels. Absolute\r\n                        path to an .xml file with the kernels description.\r\n  -api {sync,async}, --api_type {sync,async}\r\n                        Optional. Enable using sync/async API. Default value\r\n                        is async.\r\n  -niter NUMBER_ITERATIONS, --number_iterations NUMBER_ITERATIONS\r\n                        Optional. Number of iterations. If not specified, the\r\n                        number of iterations is calculated depending on a\r\n                        device.\r\n  -nireq NUMBER_INFER_REQUESTS, --number_infer_requests NUMBER_INFER_REQUESTS\r\n                        Optional. Number of infer requests. Default value is\r\n                        determined automatically for device.\r\n  -b BATCH_SIZE, --batch_size BATCH_SIZE\r\n                        Optional. Batch size value. If not specified, the\r\n                        batch size value is determined from Intermediate\r\n                        Representation\r\n  -stream_output [STREAM_OUTPUT]\r\n                        Optional. Print progress as a plain text. When\r\n                        specified, an interactive progress bar is replaced\r\n                        with a multi-line output.\r\n  -t TIME, --time TIME  Optional. Time in seconds to execute topology.\r\n  -progress [PROGRESS]  Optional. Show progress bar (can affect performance\r\n                        measurement). Default values is 'False'.\r\n  -shape SHAPE          Optional. Set shape for input. For example,\r\n                        \"input1[1,3,224,224],input2[1,4]\" or \"[1,3,224,224]\"\r\n                        in case of one input size.\r\n  -layout LAYOUT        Optional. Prompts how network layouts should be\r\n                        treated by application. For example,\r\n                        \"input1[NCHW],input2[NC]\" or \"[NCHW]\" in case of one\r\n                        input size.\r\n  -nstreams NUMBER_STREAMS, --number_streams NUMBER_STREAMS\r\n                        Optional. Number of streams to use for inference on\r\n                        the CPU/GPU/MYRIAD (for HETERO and MULTI device cases\r\n                        use format <device1>:<nstreams1>,<device2>:<nstreams2>\r\n                        or just <nstreams>). Default value is determined\r\n                        automatically for a device. Please note that although\r\n                        the automatic selection usually provides a reasonable\r\n                        performance, it still may be non - optimal for some\r\n                        cases, especially for very small networks. Also, using\r\n                        nstreams>1 is inherently throughput-oriented option,\r\n                        while for the best-latency estimations the number of\r\n                        streams should be set to 1. See samples README for\r\n                        more details.\r\n  -enforcebf16 [{True,False}], --enforce_bfloat16 [{True,False}]\r\n                        Optional. By default floating point operations\r\n                        execution in bfloat16 precision are enforced if\r\n                        supported by platform. 'true' - enable bfloat16\r\n                        regardless of platform support. 'false' - disable\r\n                        bfloat16 regardless of platform support.\r\n  -nthreads NUMBER_THREADS, --number_threads NUMBER_THREADS\r\n                        Number of threads to use for inference on the CPU, GNA\r\n                        (including HETERO and MULTI cases).\r\n  -pin {YES,NO,NUMA,HYBRID_AWARE}, --infer_threads_pinning {YES,NO,NUMA,HYBRID_AWARE}\r\n                        Optional. Enable threads->cores ('YES' which is\r\n                        OpenVINO runtime's default for conventional CPUs),\r\n                        threads->(NUMA)nodes ('NUMA'), threads->appropriate\r\n                        core types ('HYBRID_AWARE', which is OpenVINO\r\n                        runtime's default for Hybrid CPUs)or completely\r\n                        disable ('NO')CPU threads pinning for CPU-involved\r\n                        inference.\r\n  -exec_graph_path EXEC_GRAPH_PATH, --exec_graph_path EXEC_GRAPH_PATH\r\n                        Optional. Path to a file where to store executable\r\n                        graph information serialized.\r\n  -pc [PERF_COUNTS], --perf_counts [PERF_COUNTS]\r\n                        Optional. Report performance counters.\r\n  -report_type {no_counters,average_counters,detailed_counters}, --report_type {no_counters,average_counters,detailed_counters}\r\n                        Optional. Enable collecting statistics report.\r\n                        \"no_counters\" report contains configuration options\r\n                        specified, resulting FPS and latency.\r\n                        \"average_counters\" report extends \"no_counters\" report\r\n                        and additionally includes average PM counters values\r\n                        for each layer from the network. \"detailed_counters\"\r\n                        report extends \"average_counters\" report and\r\n                        additionally includes per-layer PM counters and\r\n                        latency for each executed infer request.\r\n  -report_folder REPORT_FOLDER, --report_folder REPORT_FOLDER\r\n                        Optional. Path to a folder where statistics report is\r\n                        stored.\r\n  -dump_config DUMP_CONFIG\r\n                        Optional. Path to JSON file to dump IE parameters,\r\n                        which were set by application.\r\n  -load_config LOAD_CONFIG\r\n                        Optional. Path to JSON file to load custom IE\r\n                        parameters. Please note, command line parameters have\r\n                        higher priority then parameters from configuration\r\n                        file.\r\n  -qb {8,16}, --quantization_bits {8,16}\r\n                        Optional. Weight bits for quantization: 8 (I8) or 16\r\n                        (I16)\r\n  -ip {U8,FP16,FP32}, --input_precision {U8,FP16,FP32}\r\n                        Optional. Specifies precision for all input layers of\r\n                        the network.\r\n  -op {U8,FP16,FP32}, --output_precision {U8,FP16,FP32}\r\n                        Optional. Specifies precision for all output layers of\r\n                        the network.\r\n  -iop INPUT_OUTPUT_PRECISION, --input_output_precision INPUT_OUTPUT_PRECISION\r\n                        Optional. Specifies precision for input and output\r\n                        layers by name. Example: -iop \"input:FP16,\r\n                        output:FP16\". Notice that quotes are required.\r\n                        Overwrites precision from ip and op options for\r\n                        specified layers.\r\n  -cdir CACHE_DIR, --cache_dir CACHE_DIR\r\n                        Optional. Enable model caching to specified directory\r\n  -lfile [LOAD_FROM_FILE], --load_from_file [LOAD_FROM_FILE]\r\n                        Optional. Loads model from file directly without\r\n                        read_network.\r\n", "name": "stdout"}, {"output_type": "stream", "text": "\r\nAvailable target devices:   CPU\r\n", "name": "stdout"}]}, {"metadata": {"id": "911bad7f35bb40978017463432b088f8"}, "cell_type": "markdown", "source": "## Sanity check with googlenet-v1-tf model \n### Resources:\n\n1. OpenVINO Model Zoo (OMZ): https://github.com/openvinotoolkit/open_model_zoo\n1. OMZ Intel Pre-Trained Models : https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/index.md\n1. OMZ Public Pre-Trained Models: See Column 3 for OMZ model name: https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/index.md"}, {"metadata": {"id": "bb14aaa7793b413383142b1bc383e91c", "scrolled": true}, "cell_type": "code", "source": "!python3 /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/open_model_zoo/model_tools/downloader.py --name googlenet-v1-tf", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "################|| Downloading googlenet-v1-tf ||################\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/inception_v1_2016_08_28.tar.gz\n... 100%, 24064 KB, 87307 KB/s, 0 seconds passed\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/inception.py\n... 100%, 1 KB, 3864 KB/s, 0 seconds passed\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/inception_utils.py\n... 100%, 3 KB, 5860 KB/s, 0 seconds passed\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/inception_v1.py\n... 100%, 16 KB, 23750 KB/s, 0 seconds passed\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/nets_factory.py\n... 100%, 7 KB, 20478 KB/s, 0 seconds passed\n\n========== Downloading /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/tf_slim-1.1.0-py2.py3-none-any.whl\n... 100%, 343 KB, 17014 KB/s, 0 seconds passed\n\n========== Unpacking /home/wsuser/work/public/googlenet-v1-tf/inception_v1_2016_08_28.tar.gz\n========== Unpacking /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/tf_slim-1.1.0-py2.py3-none-any.whl\n========== Replacing text in /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/nets_factory.py\n========== Replacing text in /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/nets_factory.py\n========== Replacing text in /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/inception.py\n========== Replacing text in /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/nets/nets_factory.py\n========== Replacing text in /home/wsuser/work/public/googlenet-v1-tf/models/research/slim/tf_slim/layers/layers.py\n\n", "name": "stdout"}]}, {"metadata": {"id": "0f9c1cb31a874f8db5d37b550d5b45d3"}, "cell_type": "code", "source": "!ls public/googlenet-v1-tf/", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "FP16  FP32  inception_v1.ckpt  inception_v1.frozen.pb  models\r\n", "name": "stdout"}]}, {"metadata": {"id": "97f8ea53e843446c83f45c797a04c50f"}, "cell_type": "code", "source": "!python3 /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/open_model_zoo/model_tools/converter.py --name googlenet-v1-tf\n\n", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "========== Running pre-convert script for googlenet-v1-tf\nPre-convert command: /opt/conda/envs/Python-3.6-WMLCE/bin/python3 -- /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/open_model_zoo/model_tools/models/public/googlenet-v1-tf/pre-convert.py -- /home/wsuser/work/public/googlenet-v1-tf /home/wsuser/work/public/googlenet-v1-tf\n\n2021-07-06 17:22:26.580873: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2021-07-06 17:22:26.595795: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2394380000 Hz\n2021-07-06 17:22:26.601898: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562dec538a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2021-07-06 17:22:26.602006: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\nWARNING: Logging before flag parsing goes to stderr.\nW0706 17:22:29.227812 140697302361920 deprecation.py:323] From /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/open_model_zoo/model_tools/models/public/googlenet-v1-tf/pre-convert.py:45: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\nW0706 17:22:29.228379 140697302361920 deprecation.py:323] From /opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\n\n========== Converting googlenet-v1-tf to IR (FP16)\nConversion command: /opt/conda/envs/Python-3.6-WMLCE/bin/python3 -m mo --framework=tf --data_type=FP16 --output_dir=/home/wsuser/work/public/googlenet-v1-tf/FP16 --model_name=googlenet-v1-tf '--input_shape=[1,224,224,3]' --input=input '--mean_values=input[127.5,127.5,127.5]' '--scale_values=input[127.5]' --output=InceptionV1/Logits/Predictions/Softmax --input_model=/home/wsuser/work/public/googlenet-v1-tf/inception_v1.frozen.pb --reverse_input_channels\n\nModel Optimizer arguments:\nCommon parameters:\n\t- Path to the Input Model: \t/home/wsuser/work/public/googlenet-v1-tf/inception_v1.frozen.pb\n\t- Path for generated IR: \t/home/wsuser/work/public/googlenet-v1-tf/FP16\n\t- IR output name: \tgooglenet-v1-tf\n\t- Log level: \tERROR\n\t- Batch: \tNot specified, inherited from the model\n\t- Input layers: \tinput\n\t- Output layers: \tInceptionV1/Logits/Predictions/Softmax\n\t- Input shapes: \t[1,224,224,3]\n\t- Mean values: \tinput[127.5,127.5,127.5]\n\t- Scale values: \tinput[127.5]\n\t- Scale factor: \tNot specified\n\t- Precision of IR: \tFP16\n\t- Enable fusing: \tTrue\n\t- Enable grouped convolutions fusing: \tTrue\n\t- Move mean values to preprocess section: \tNone\n\t- Reverse input channels: \tTrue\nTensorFlow specific parameters:\n\t- Input model in text protobuf format: \tFalse\n\t- Path to model dump for TensorBoard: \tNone\n\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n\t- Update the configuration file with input/output node names: \tNone\n\t- Use configuration file used to generate the model with Object Detection API: \tNone\n\t- Use the config file: \tNone\n\t- Inference Engine found in: \t/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/openvino\nInference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\nModel Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n[ SUCCESS ] Generated IR version 10 model.\n[ SUCCESS ] XML file: /home/wsuser/work/public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml\n[ SUCCESS ] BIN file: /home/wsuser/work/public/googlenet-v1-tf/FP16/googlenet-v1-tf.bin\n[ SUCCESS ] Total execution time: 28.73 seconds. \n[ SUCCESS ] Memory consumed: 420 MB. \n\n========== Converting googlenet-v1-tf to IR (FP32)\nConversion command: /opt/conda/envs/Python-3.6-WMLCE/bin/python3 -m mo --framework=tf --data_type=FP32 --output_dir=/home/wsuser/work/public/googlenet-v1-tf/FP32 --model_name=googlenet-v1-tf '--input_shape=[1,224,224,3]' --input=input '--mean_values=input[127.5,127.5,127.5]' '--scale_values=input[127.5]' --output=InceptionV1/Logits/Predictions/Softmax --input_model=/home/wsuser/work/public/googlenet-v1-tf/inception_v1.frozen.pb --reverse_input_channels\n\nModel Optimizer arguments:\nCommon parameters:\n\t- Path to the Input Model: \t/home/wsuser/work/public/googlenet-v1-tf/inception_v1.frozen.pb\n\t- Path for generated IR: \t/home/wsuser/work/public/googlenet-v1-tf/FP32\n\t- IR output name: \tgooglenet-v1-tf\n\t- Log level: \tERROR\n\t- Batch: \tNot specified, inherited from the model\n\t- Input layers: \tinput\n\t- Output layers: \tInceptionV1/Logits/Predictions/Softmax\n\t- Input shapes: \t[1,224,224,3]\n\t- Mean values: \tinput[127.5,127.5,127.5]\n\t- Scale values: \tinput[127.5]\n\t- Scale factor: \tNot specified\n\t- Precision of IR: \tFP32\n\t- Enable fusing: \tTrue\n\t- Enable grouped convolutions fusing: \tTrue\n\t- Move mean values to preprocess section: \tNone\n\t- Reverse input channels: \tTrue\nTensorFlow specific parameters:\n\t- Input model in text protobuf format: \tFalse\n\t- Path to model dump for TensorBoard: \tNone\n\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n\t- Update the configuration file with input/output node names: \tNone\n\t- Use configuration file used to generate the model with Object Detection API: \tNone\n\t- Use the config file: \tNone\n\t- Inference Engine found in: \t/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/openvino\nInference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\nModel Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n[ SUCCESS ] Generated IR version 10 model.\n[ SUCCESS ] XML file: /home/wsuser/work/public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml\n[ SUCCESS ] BIN file: /home/wsuser/work/public/googlenet-v1-tf/FP32/googlenet-v1-tf.bin\n[ SUCCESS ] Total execution time: 29.63 seconds. \n[ SUCCESS ] Memory consumed: 423 MB. \n\n", "name": "stdout"}]}, {"metadata": {"id": "96711ed8d70540618b0ac8af8999b8e3"}, "cell_type": "code", "source": "!benchmark_app -m /home/wsuser/work/public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "[Step 1/11] Parsing and validating input arguments\n[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n[Step 2/11] Loading Inference Engine\n[ INFO ] InferenceEngine:\n         API version............. 2021.4.0-3839-cd81789d294-releases/2021/4\n[ INFO ] Device info\n         CPU\n         MKLDNNPlugin............ version 2.1\n         Build................... 2021.4.0-3839-cd81789d294-releases/2021/4\n\n[Step 3/11] Setting device configuration\n[ WARNING ] -nstreams default value is determined automatically for CPU device. Although the automatic selection usually provides a reasonable performance,but it still may be non-optimal for some cases, for more information look at README.\n[Step 4/11] Reading network files\n[ INFO ] Read network took 46.25 ms\n[Step 5/11] Resizing network to match image sizes and given batch\n[ INFO ] Network batch size: 1\n[Step 6/11] Configuring input of the model\n[ INFO ] Network input 'input' precision U8, dimensions (NCHW): 1 3 224 224\n[ INFO ] Network output 'InceptionV1/Logits/Predictions/Softmax' precision FP32, dimensions (NC): 1 1001\n[Step 7/11] Loading the model to the device\n[ INFO ] Load network took 1094.77 ms\n[Step 8/11] Setting optimal runtime parameters\n[Step 9/11] Creating infer requests and filling input blobs with images\n[ WARNING ] No input files were given: all inputs will be filled with random values!\n[ INFO ] Infer Request 0 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 1 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 2 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 3 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 4 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 5 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 6 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[ INFO ] Infer Request 7 filling\n[ INFO ] Fill input 'input' with random values (image is expected)\n[Step 10/11] Measuring performance (Start inference asynchronously, 8 inference requests using 8 streams for CPU, limits: 60000 ms duration)\n[ INFO ] First inference took 53.99 ms\n[Step 11/11] Dumping statistics report\nCount:      720 iterations\nDuration:   60714.68 ms\nLatency:    696.64 ms\nThroughput: 11.86 FPS\n", "name": "stdout"}]}, {"metadata": {"id": "123e7b28deba4d8babd79b8766a38091"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}
